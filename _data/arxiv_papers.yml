papers:
- id: '2602.15827'
  title: 'Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching'
  authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi
    Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi,
    C. Karen Liu
  abstract: 'While recent advances in humanoid locomotion have achieved stable walking
    on varied terrains, capturing the agility and adaptivity of highly dynamic human
    motions remains an open challenge. In particular, agile parkour in complex environments
    demands not only low-level robustness, but also human-like motion expressiveness,
    long-horizon skill composition, and perception-driven decision-making. In this
    paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that
    enables humanoid robots to autonomously perform long-horizon, vision-based parkour
    across challenging obstacle courses. Our approach first leverages motion matching,
    formulated as nearest-neighbor search in a feature space, to compose retargeted
    atomic human skills into long-horizon kinematic trajectories. This framework enables
    the flexible composition and smooth transition of complex skill chains while preserving
    the elegance and fluidity of dynamic human motions. Next, we train motion-tracking
    reinforcement learning (RL) expert policies for these composed motions, and distill
    them into a single depth-based, multi-skill student policy, using a combination
    of DAgger and RL. Crucially, the combination of perception and skill composition
    enables autonomous, context-aware decision-making: using only onboard depth sensing
    and a discrete 2D velocity command, the robot selects and executes whether to
    step over, climb onto, vault or roll off obstacles of varying geometries and heights.
    We validate our framework with extensive real-world experiments on a Unitree G1
    humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall
    obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle
    traversal with closed-loop adaptation to real-time obstacle perturbations.'
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15827
  arxiv_url: http://arxiv.org/abs/2602.15827
  key_findings: While recent advances in humanoid locomotion have achieved stable
    walking on varied terrains, capturing the agility and adaptivity of highly dynamic
    human motions remains an open challenge. In particular, agile parkour in complex
    environments demands not only low-level robustness, but also human-like motion
    expressiveness, long-horizon skill composition, and perception-driven decision-making.
    In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework
    that enables humanoid robots to autonomously perform long-horizon, vision-based
    parkour across challenging obstacle courses.
- id: '2602.15823'
  title: 'CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing'
  authors: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria
    Rashidinejad
  abstract: 'A central challenge in large language model (LLM) editing is capability
    preservation: methods that successfully change targeted behavior can quietly game
    the editing proxy and corrupt general capabilities, producing degenerate behaviors
    reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled
    second-order editing algorithm that treats capability preservation as an explicit
    constraint, unifying and generalizing several existing editing approaches. CrispEdit
    formulates editing as constrained optimization and enforces the constraint by
    projecting edit updates onto the low-curvature subspace of the capability-loss
    landscape. At the crux of CrispEdit is expressing capability constraint via Bregman
    divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even
    when the base model is not trained to convergence. We make this second-order procedure
    efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC)
    and a novel matrix-free projector that exploits Kronecker structure to avoid constructing
    massive projection matrices. Across standard model-editing benchmarks, CrispEdit
    achieves high edit success while keeping capability degradation below 1% on average
    across datasets, significantly improving over prior editors.'
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15823
  arxiv_url: http://arxiv.org/abs/2602.15823
  key_findings: 'A central challenge in large language model (LLM) editing is capability
    preservation: methods that successfully change targeted behavior can quietly game
    the editing proxy and corrupt general capabilities, producing degenerate behaviors
    reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled
    second-order editing algorithm that treats capability preservation as an explicit
    constraint, unifying and generalizing several existing editing approaches. CrispEdit
    formulates editing as constrained optimization and enforces the constraint by
    projecting edit updates onto the low-curvature subspace of the capability-loss
    landscape.'
- id: '2602.15816'
  title: 'Developing AI Agents with Simulated Data: Why, what, and how?'
  authors: Xiaoran Liu, Istvan David
  abstract: As insufficient data volume and quality remain the key impediments to
    the adoption of modern subsymbolic AI, techniques of synthetic data generation
    are in high demand. Simulation offers an apt, systematic approach to generating
    diverse synthetic data. This chapter introduces the reader to the key concepts,
    benefits, and challenges of simulation-based synthetic data generation for AI
    training purposes, and to a reference framework to describe, design, and analyze
    digital twin-based AI simulation solutions.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15816
  arxiv_url: http://arxiv.org/abs/2602.15816
  key_findings: As insufficient data volume and quality remain the key impediments
    to the adoption of modern subsymbolic AI, techniques of synthetic data generation
    are in high demand. Simulation offers an apt, systematic approach to generating
    diverse synthetic data. This chapter introduces the reader to the key concepts,
    benefits, and challenges of simulation-based synthetic data generation for AI
    training purposes, and to a reference framework to describe, design, and analyze
    digital twin-based AI simulation solutions.
- id: '2602.15814'
  title: Avey-B
  authors: Devang Acharya, Mohammad Hammoud
  abstract: Compact pretrained bidirectional encoders remain the backbone of industrial
    NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's
    ability to deliver high-quality bidirectional contextualization with sequence-level
    parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced
    as an autoregressive, attention-free alternative that naturally admits an encoder-only
    adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and
    propose several innovations to its architecture, including decoupled static and
    dynamic parameterizations, stability-oriented normalization, and neural compression.
    Results show that this reformulated architecture compares favorably to four widely
    used Transformer-based encoders, consistently outperforming them on standard token-classification
    and information-retrieval benchmarks while scaling more efficiently to long contexts.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15814
  arxiv_url: http://arxiv.org/abs/2602.15814
  key_findings: Compact pretrained bidirectional encoders remain the backbone of industrial
    NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's
    ability to deliver high-quality bidirectional contextualization with sequence-level
    parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced
    as an autoregressive, attention-free alternative that naturally admits an encoder-only
    adaptation.
- id: '2602.15811'
  title: Task-Agnostic Continual Learning for Chest Radiograph Classification
  authors: Muthu Subash Kavitha, Anas Zafar, Amgad Muneer, Jia Wu
  abstract: Clinical deployment of chest radiograph classifiers requires models that
    can be updated as new datasets become available without retraining on previously
    ob- served data or degrading validated performance. We study, for the first time,
    a task-incremental continual learning setting for chest radiograph classification,
    in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers
    are unavailable at inference. We propose a continual adapter-based routing learning
    strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone
    and incrementally allocates lightweight task-specific adapters and classifier
    heads. A latent task selector operates on task-adapted features and leverages
    both current and historical context preserved through compact prototypes and feature-level
    experience replay. This design supports stable task identification and adaptation
    across sequential updates while avoiding raw-image storage. Experiments on large-scale
    public chest radiograph datasets demonstrate robust performance retention and
    reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms
    joint training under task-unknown deployment, achieving higher routing accuracy
    (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with
    AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under
    task-unknown inference, using significantly fewer trainable parameters. Finally,
    the proposed framework provides a practical alternative to joint training and
    repeated full retraining in continual clinical deployment.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15811
  arxiv_url: http://arxiv.org/abs/2602.15811
  key_findings: Clinical deployment of chest radiograph classifiers requires models
    that can be updated as new datasets become available without retraining on previously
    ob- served data or degrading validated performance. We study, for the first time,
    a task-incremental continual learning setting for chest radiograph classification,
    in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers
    are unavailable at inference. We propose a continual adapter-based routing learning
    strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone
    and incrementally allocates lightweight task-specific adapters and classifier
    heads.
- id: '2602.15809'
  title: Decision Quality Evaluation Framework at Pinterest
  authors: Yuqi Tian, Robert Paine, Attila Dobi, Kevin O'Sullivan, Aravindh Manickavasagam,
    Faisal Farooq
  abstract: 'Online platforms require robust systems to enforce content safety policies
    at scale. A critical component of these systems is the ability to evaluate the
    quality of moderation decisions made by both human agents and Large Language Models
    (LLMs). However, this evaluation is challenging due to the inherent trade-offs
    between cost, scale, and trustworthiness, along with the complexity of evolving
    policies. To address this, we present a comprehensive Decision Quality Evaluation
    Framework developed and deployed at Pinterest. The framework is centered on a
    high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves
    as a ground truth benchmark. We introduce an automated intelligent sampling pipeline
    that uses propensity scores to efficiently expand dataset coverage. We demonstrate
    the framework''s practical application in several key areas: benchmarking the
    cost-performance trade-offs of various LLM agents, establishing a rigorous methodology
    for data-driven prompt optimization, managing complex policy evolution, and ensuring
    the integrity of policy content prevalence metrics via continuous validation.
    The framework enables a shift from subjective assessments to a data-driven and
    quantitative practice for managing content safety systems.'
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15809
  arxiv_url: http://arxiv.org/abs/2602.15809
  key_findings: Online platforms require robust systems to enforce content safety
    policies at scale. A critical component of these systems is the ability to evaluate
    the quality of moderation decisions made by both human agents and Large Language
    Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs
    between cost, scale, and trustworthiness, along with the complexity of evolving
    policies.
- id: '2602.15799'
  title: 'The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety'
  authors: Max Springer, Chung Peng Lee, Blossom Metevier, Jane Castleman, Bohdan
    Turbal, Hayoung Jung, Zeyu Shen, Aleksandra Korolova
  abstract: 'Fine-tuning aligned language models on benign tasks unpredictably degrades
    safety guardrails, even when training data contains no harmful content and developers
    have no adversarial intent. We show that the prevailing explanation, that fine-tuning
    updates should be orthogonal to safety-critical directions in high-dimensional
    parameter space, offers false reassurance: we show this orthogonality is structurally
    unstable and collapses under the dynamics of gradient descent. We then resolve
    this through a novel geometric analysis, proving that alignment concentrates in
    low-dimensional subspaces with sharp curvature, creating a brittle structure that
    first-order methods cannot detect or defend. While initial fine-tuning updates
    may indeed avoid these subspaces, the curvature of the fine-tuning loss generates
    second-order acceleration that systematically steers trajectories into alignment-sensitive
    regions. We formalize this mechanism through the Alignment Instability Condition,
    three geometric properties that, when jointly satisfied, lead to safety degradation.
    Our main result establishes a quartic scaling law: alignment loss grows with the
    fourth power of training time, governed by the sharpness of alignment geometry
    and the strength of curvature coupling between the fine-tuning task and safety-critical
    parameters. These results expose a structural blind spot in the current safety
    paradigm. The dominant approaches to safe fine-tuning address only the initial
    snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug
    to be patched; it is an intrinsic geometric property of gradient descent on curved
    manifolds. Our results motivate the development of curvature-aware methods, and
    we hope will further enable a shift in alignment safety analysis from reactive
    red-teaming to predictive diagnostics for open-weight model deployment.'
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15799
  arxiv_url: http://arxiv.org/abs/2602.15799
  key_findings: 'Fine-tuning aligned language models on benign tasks unpredictably
    degrades safety guardrails, even when training data contains no harmful content
    and developers have no adversarial intent. We show that the prevailing explanation,
    that fine-tuning updates should be orthogonal to safety-critical directions in
    high-dimensional parameter space, offers false reassurance: we show this orthogonality
    is structurally unstable and collapses under the dynamics of gradient descent.
    We then resolve this through a novel geometric analysis, proving that alignment
    concentrates in low-dimensional subspaces with sharp curvature, creating a brittle
    structure that first-order methods cannot detect or defend.'
- id: '2602.15791'
  title: Enhancing Building Semantics Preservation in AI Model Training with Large
    Language Model Encodings
  authors: Suhyung Jang, Ghang Lee, Jaekun Lee, Hyunjun Lee
  abstract: Accurate representation of building semantics, encompassing both generic
    object types and specific subtypes, is essential for effective AI model training
    in the architecture, engineering, construction, and operation (AECO) industry.
    Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced
    relationships among closely related subtypes, limiting AI's semantic comprehension.
    To address this limitation, this study proposes a novel training approach that
    employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA)
    as encodings to preserve finer distinctions in building semantics. We evaluated
    the proposed method by training GraphSAGE models to classify 42 building object
    subtypes across five high-rise residential building information models (BIMs).
    Various embedding dimensions were tested, including original high-dimensional
    LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings
    generated via the Matryoshka representation model. Experimental results demonstrated
    that LLM encodings outperformed the conventional one-hot baseline, with the llama-3
    (compacted) embedding achieving a weighted average F1-score of 0.8766, compared
    to 0.8475 for one-hot encoding. The results underscore the promise of leveraging
    LLM-based encodings to enhance AI's ability to interpret complex, domain-specific
    building semantics. As the capabilities of LLMs and dimensionality reduction techniques
    continue to evolve, this approach holds considerable potential for broad application
    in semantic elaboration tasks throughout the AECO industry.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15791
  arxiv_url: http://arxiv.org/abs/2602.15791
  key_findings: Accurate representation of building semantics, encompassing both generic
    object types and specific subtypes, is essential for effective AI model training
    in the architecture, engineering, construction, and operation (AECO) industry.
    Conventional encoding methods (e. g.
- id: '2602.15785'
  title: 'This human study did not involve human subjects: Validating LLM simulations
    as behavioral evidence'
  authors: Jessica Hullman, David Broska, Huaman Sun, Aaron Shaw
  abstract: A growing literature uses large language models (LLMs) as synthetic participants
    to generate cost-effective and nearly instantaneous responses in social science
    experiments. However, there is limited guidance on when such simulations support
    valid inference about human behavior. We contrast two strategies for obtaining
    valid estimates of causal effects and clarify the assumptions under which each
    is suitable for exploratory versus confirmatory research. Heuristic approaches
    seek to establish that simulated and observed human behavior are interchangeable
    through prompt engineering, model fine-tuning, and other repair strategies designed
    to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic
    approaches lack the formal statistical guarantees typically required for confirmatory
    research. In contrast, statistical calibration combines auxiliary human data with
    statistical adjustments to account for discrepancies between observed and simulated
    responses. Under explicit assumptions, statistical calibration preserves validity
    and provides more precise estimates of causal effects at lower cost than experiments
    that rely solely on human participants. Yet the potential of both approaches depends
    on how well LLMs approximate the relevant populations. We consider what opportunities
    are overlooked when researchers focus myopically on substituting LLMs for human
    participants in a study.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15785
  arxiv_url: http://arxiv.org/abs/2602.15785
  key_findings: A growing literature uses large language models (LLMs) as synthetic
    participants to generate cost-effective and nearly instantaneous responses in
    social science experiments. However, there is limited guidance on when such simulations
    support valid inference about human behavior. We contrast two strategies for obtaining
    valid estimates of causal effects and clarify the assumptions under which each
    is suitable for exploratory versus confirmatory research.
- id: '2602.15776'
  title: 'GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent
    Systems'
  authors: Yiqin Yang, Xu Yang, Yuhua Jiang, Ni Mu, Hao Hu, Runpeng Xie, Ziyou Zhang,
    Siyuan Li, Yuan-Hua Ni, Qianchuan Zhao, Bo Xu
  abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability}
    is a critical barrier to effective coordination and decision-making. Existing
    approaches, such as belief state estimation and inter-agent communication, often
    fall short. Belief-based methods are limited by their focus on past experiences
    without fully leveraging global information, while communication methods often
    lack a robust model to effectively utilize the auxiliary information they provide.
    To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to
    infer the global state based on the local observations. By formulating the state
    inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities
    in state estimation while simultaneously inferring the global state with high
    fidelity. We prove that the estimation error of GlobeDiff under both unimodal
    and multi-modal distributions can be bounded. Extensive experimental results demonstrate
    that GlobeDiff achieves superior performance and is capable of accurately inferring
    the global state.
  published: '2026-02-17'
  pdf_url: http://arxiv.org/pdf/2602.15776
  arxiv_url: http://arxiv.org/abs/2602.15776
  key_findings: In the realm of multi-agent systems, the challenge of \emph{partial
    observability} is a critical barrier to effective coordination and decision-making.
    Existing approaches, such as belief state estimation and inter-agent communication,
    often fall short. Belief-based methods are limited by their focus on past experiences
    without fully leveraging global information, while communication methods often
    lack a robust model to effectively utilize the auxiliary information they provide.
