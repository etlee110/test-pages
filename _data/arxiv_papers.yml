papers:
- id: '2602.13194'
  title: Semantic Chunking and the Entropy of Natural Language
  authors: Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks
  abstract: The entropy rate of printed English is famously estimated to be about
    one bit per character, a benchmark that modern large language models (LLMs) have
    only recently approached. This entropy rate implies that English contains nearly
    80 percent redundancy relative to the five bits per character expected for random
    text. We introduce a statistical model that attempts to capture the intricate
    multi-scale structure of natural language, providing a first-principles account
    of this redundancy level. Our model describes a procedure of self-similarly segmenting
    text into semantically coherent chunks down to the single-word level. The semantic
    structure of the text can then be hierarchically decomposed, allowing for analytical
    treatment. Numerical experiments with modern LLMs and open datasets suggest that
    our model quantitatively captures the structure of real texts at different levels
    of the semantic hierarchy. The entropy rate predicted by our model agrees with
    the estimated entropy rate of printed English. Moreover, our theory further reveals
    that the entropy rate of natural language is not fixed but should increase systematically
    with the semantic complexity of corpora, which are captured by the only free parameter
    in our model.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13194
  arxiv_url: http://arxiv.org/abs/2602.13194
  key_findings: The entropy rate of printed English is famously estimated to be about
    one bit per character, a benchmark that modern large language models (LLMs) have
    only recently approached. This entropy rate implies that English contains nearly
    80 percent redundancy relative to the five bits per character expected for random
    text. We introduce a statistical model that attempts to capture the intricate
    multi-scale structure of natural language, providing a first-principles account
    of this redundancy level.
- id: '2602.13191'
  title: 'CoPE-VideoLM: Codec Primitives For Efficient Video Language Models'
  authors: Sayan Deb Sarkar, Rémi Pautrat, Ondrej Miksik, Marc Pollefeys, Iro Armeni,
    Mahdi Rad, Mihai Dusmanu
  abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal
    dynamics in videos. To fit to the maximum context window constraint, current methods
    use keyframe sampling which can miss both macro-level events and micro-level details
    due to the sparse temporal coverage. Furthermore, processing full images and their
    tokens for each frame incurs substantial computational overhead. To address these
    limitations, we propose to leverage video codec primitives (specifically motion
    vectors and residuals) which natively encode video redundancy and sparsity without
    requiring expensive full-image encoding for most frames. To this end, we introduce
    lightweight transformer-based encoders that aggregate codec primitives and align
    their representations with image encoder embeddings through a pre-training strategy
    that accelerates convergence during end-to-end fine-tuning. Our approach reduces
    the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared
    to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities
    we are able to maintain or exceed performance on $14$ diverse video understanding
    benchmarks spanning general question answering, temporal reasoning, long-form
    understanding, and spatial scene understanding.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13191
  arxiv_url: http://arxiv.org/abs/2602.13191
  key_findings: Video Language Models (VideoLMs) empower AI systems to understand
    temporal dynamics in videos. To fit to the maximum context window constraint,
    current methods use keyframe sampling which can miss both macro-level events and
    micro-level details due to the sparse temporal coverage. Furthermore, processing
    full images and their tokens for each frame incurs substantial computational overhead.
- id: '2602.13166'
  title: Optimal Take-off under Fuzzy Clearances
  authors: Hugo Henry, Arthur Tsai, Kelly Cohen
  abstract: This paper presents a hybrid obstacle avoidance architecture that integrates
    Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable
    adaptive constraint handling for unmanned aircraft. Motivated by the limitations
    of classical optimal control under uncertainty and the need for interpretable
    decision making in safety critical aviation systems, we design a three stage Takagi
    Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation
    decisions based on regulatory separation minima and airworthiness guidelines from
    FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints
    into an optimal control problem solved using the FALCON toolbox and IPOPT. The
    framework aims to reduce unnecessary recomputations by selectively activating
    obstacle avoidance updates while maintaining compliance with aviation procedures.
    A proof of concept implementation using a simplified aircraft model demonstrates
    that the approach can generate optimal trajectories with computation times of
    2,3 seconds per iteration in a single threaded MATLAB environment, suggesting
    feasibility for near real time applications. However, our experiments revealed
    a critical software incompatibility in the latest versions of FALCON and IPOPT,
    in which the Lagrangian penalty term remained identically zero, preventing proper
    constraint enforcement. This behavior was consistent across scenarios and indicates
    a solver toolbox regression rather than a modeling flaw. Future work includes
    validating this effect by reverting to earlier software versions, optimizing the
    fuzzy membership functions using evolutionary methods, and extending the system
    to higher fidelity aircraft models and stochastic obstacle environments.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13166
  arxiv_url: http://arxiv.org/abs/2602.13166
  key_findings: This paper presents a hybrid obstacle avoidance architecture that
    integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS)
    to enable adaptive constraint handling for unmanned aircraft. Motivated by the
    limitations of classical optimal control under uncertainty and the need for interpretable
    decision making in safety critical aviation systems, we design a three stage Takagi
    Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation
    decisions based on regulatory separation minima and airworthiness guidelines from
    FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints
    into an optimal control problem solved using the FALCON toolbox and IPOPT.
- id: '2602.13165'
  title: Asynchronous Verified Semantic Caching for Tiered LLM Architectures
  authors: Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, Tak Chiam,
    Weihua Zhu
  abstract: 'Large language models (LLMs) now sit in the critical path of search,
    assistance, and agentic workflows, making semantic caching essential for reducing
    inference cost and latency. Production deployments typically use a tiered static-dynamic
    design: a static cache of curated, offline vetted responses mined from logs, backed
    by a dynamic cache populated online. In practice, both tiers are commonly governed
    by a single embedding similarity threshold, which induces a hard tradeoff: conservative
    thresholds miss safe reuse opportunities, while aggressive thresholds risk serving
    semantically incorrect responses. We introduce \textbf{Krites}, an asynchronous,
    LLM-judged caching policy that expands static coverage without changing serving
    decisions. On the critical path, Krites behaves exactly like a standard static
    threshold policy. When the nearest static neighbor of the prompt falls just below
    the static threshold, Krites asynchronously invokes an LLM judge to verify whether
    the static response is acceptable for the new prompt. Approved matches are promoted
    into the dynamic cache, allowing future repeats and paraphrases to reuse curated
    static answers and expanding static reach over time. In trace-driven simulations
    on conversational and search workloads, Krites increases the fraction of requests
    served with curated static answers (direct static hits plus verified promotions)
    by up to $\textbf{3.9}$ times for conversational traffic and search-style queries
    relative to tuned baselines, with unchanged critical path latency.'
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13165
  arxiv_url: http://arxiv.org/abs/2602.13165
  key_findings: 'Large language models (LLMs) now sit in the critical path of search,
    assistance, and agentic workflows, making semantic caching essential for reducing
    inference cost and latency. Production deployments typically use a tiered static-dynamic
    design: a static cache of curated, offline vetted responses mined from logs, backed
    by a dynamic cache populated online. In practice, both tiers are commonly governed
    by a single embedding similarity threshold, which induces a hard tradeoff: conservative
    thresholds miss safe reuse opportunities, while aggressive thresholds risk serving
    semantically incorrect responses.'
- id: '2602.13156'
  title: 'In-Context Autonomous Network Incident Response: An End-to-End Large Language
    Model Agent Approach'
  authors: Yiran Gao, Kim Hammar, Tao Li
  abstract: Rapidly evolving cyberattacks demand incident response systems that can
    autonomously learn and adapt to changing threats. Prior work has extensively explored
    the reinforcement learning approach, which involves learning response strategies
    through extensive simulation of the incident. While this approach can be effective,
    it requires handcrafted modeling of the simulator and suppresses useful semantics
    from raw system logs and alerts. To address these limitations, we propose to leverage
    large language models' (LLM) pre-trained security knowledge and in-context learning
    to create an end-to-end agentic solution for incident response planning. Specifically,
    our agent integrates four functionalities, perception, reasoning, planning, and
    action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought
    reasoning, our LLM agent is capable of processing system logs and inferring the
    underlying network state (perception), updating its conjecture of attack models
    (reasoning), simulating consequences under different response strategies (planning),
    and generating an effective response (action). By comparing LLM-simulated outcomes
    with actual observations, the LLM agent repeatedly refines its attack conjecture
    and corresponding response, thereby demonstrating in-context adaptation. Our agentic
    approach is free of modeling and can run on commodity hardware. When evaluated
    on incident logs reported in the literature, our agent achieves recovery up to
    23% faster than those of frontier LLMs.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13156
  arxiv_url: http://arxiv.org/abs/2602.13156
  key_findings: Rapidly evolving cyberattacks demand incident response systems that
    can autonomously learn and adapt to changing threats. Prior work has extensively
    explored the reinforcement learning approach, which involves learning response
    strategies through extensive simulation of the incident. While this approach can
    be effective, it requires handcrafted modeling of the simulator and suppresses
    useful semantics from raw system logs and alerts.
- id: '2602.13135'
  title: Constrained Assumption-Based Argumentation Frameworks
  authors: Emanuele De Angelis, Fabio Fioravanti, Maria Chiara Meo, Alberto Pettorossi,
    Maurizio Proietti, Francesca Toni
  abstract: Assumption-based Argumentation (ABA) is a well-established form of structured
    argumentation. ABA frameworks with an underlying atomic language are widely studied,
    but their applicability is limited by a representational restriction to ground
    (variable-free) arguments and attacks built from propositional atoms. In this
    paper, we lift this restriction and propose a novel notion of constrained ABA
    (CABA), whose components, as well as arguments built from them, may include constrained
    variables, ranging over possibly infinite domains. We define non-ground semantics
    for CABA, in terms of various notions of non-ground attacks. We show that the
    new semantics conservatively generalise standard ABA semantics.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13135
  arxiv_url: http://arxiv.org/abs/2602.13135
  key_findings: Assumption-based Argumentation (ABA) is a well-established form of
    structured argumentation. ABA frameworks with an underlying atomic language are
    widely studied, but their applicability is limited by a representational restriction
    to ground (variable-free) arguments and attacks built from propositional atoms.
    In this paper, we lift this restriction and propose a novel notion of constrained
    ABA (CABA), whose components, as well as arguments built from them, may include
    constrained variables, ranging over possibly infinite domains.
- id: '2602.13110'
  title: 'SCOPE: Selective Conformal Optimized Pairwise LLM Judging'
  authors: Sher Badshah, Ali Emami, Hassan Sajjad
  abstract: Large language models (LLMs) are increasingly used as judges to replace
    costly human preference labels in pairwise evaluation. Despite their practicality,
    LLM judges remain prone to miscalibration and systematic biases. This paper proposes
    SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective
    pairwise judging with finite-sample statistical guarantees. Under exchangeability,
    SCOPE calibrates an acceptance threshold such that the error rate among non-abstained
    judgments is at most a user-specified level $α$. To provide SCOPE with a bias-neutral
    uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which
    queries the judge under both response positions, aggregates the implied preference
    probabilities to enforce invariance to response order, and converts the aggregated
    probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench,
    and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies,
    providing a stronger selection signal that enables SCOPE to consistently meet
    the target risk level while retaining good coverage across judge scales. In particular,
    at $α= 0.10$, \textsc{Scope} consistently satisfies the risk bound across all
    benchmarks and judge scales (empirical risk $\approx 0.097$ to $0.099$), while
    retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and
    $0.98$ on RewardBench with Qwen-32B. Compared to naïve baselines, \textsc{Scope}
    accepts up to $2.4\times$ more judgments on MT-Bench with Qwen-7B under the same
    target risk constraint, demonstrating that BPE enables reliable and high-coverage
    LLM-based evaluation.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13110
  arxiv_url: http://arxiv.org/abs/2602.13110
  key_findings: Large language models (LLMs) are increasingly used as judges to replace
    costly human preference labels in pairwise evaluation. Despite their practicality,
    LLM judges remain prone to miscalibration and systematic biases. This paper proposes
    SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective
    pairwise judging with finite-sample statistical guarantees.
- id: '2602.13106'
  title: Which Algorithms Can Graph Neural Networks Learn?
  authors: Solveig Wittig, Antonis Vasileiou, Robert R. Nerem, Timo Stoll, Floris
    Geerts, Yusu Wang, Christopher Morris
  abstract: In recent years, there has been growing interest in understanding neural
    architectures' ability to learn to execute discrete algorithms, a line of work
    often referred to as neural algorithmic reasoning. The goal is to integrate algorithmic
    reasoning capabilities into larger neural pipelines. Many such architectures are
    based on (message-passing) graph neural networks (MPNNs), owing to their permutation
    equivariance and ability to deal with sparsity and variable-sized inputs. However,
    existing work is either largely empirical and lacks formal guarantees or it focuses
    solely on expressivity, leaving open the question of when and how such architectures
    generalize beyond a finite training set. In this work, we propose a general theoretical
    framework that characterizes the sufficient conditions under which MPNNs can learn
    an algorithm from a training set of small instances and provably approximate its
    behavior on inputs of arbitrary size. Our framework applies to a broad class of
    algorithms, including single-source shortest paths, minimum spanning trees, and
    general dynamic programming problems, such as the $0$-$1$ knapsack problem. In
    addition, we establish impossibility results for a wide range of algorithmic tasks,
    showing that standard MPNNs cannot learn them, and we derive more expressive MPNN-like
    architectures that overcome these limitations. Finally, we refine our analysis
    for the Bellman-Ford algorithm, yielding a substantially smaller required training
    set and significantly extending the recent work of Nerem et al. [2025] by allowing
    for a differentiable regularization loss. Empirical results largely support our
    theoretical findings.
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13106
  arxiv_url: http://arxiv.org/abs/2602.13106
  key_findings: In recent years, there has been growing interest in understanding
    neural architectures' ability to learn to execute discrete algorithms, a line
    of work often referred to as neural algorithmic reasoning. The goal is to integrate
    algorithmic reasoning capabilities into larger neural pipelines. Many such architectures
    are based on (message-passing) graph neural networks (MPNNs), owing to their permutation
    equivariance and ability to deal with sparsity and variable-sized inputs.
- id: '2602.13093'
  title: Consistency of Large Reasoning Models Under Multi-Turn Attacks
  authors: Yubo Li, Ramayya Krishnan, Rema Padman
  abstract: 'Large reasoning models with reasoning capabilities achieve state-of-the-art
    performance on complex tasks, but their robustness under multi-turn adversarial
    pressure remains underexplored. We evaluate nine frontier reasoning models under
    adversarial attacks. Our findings reveal that reasoning confers meaningful but
    incomplete robustness: most reasoning models studied significantly outperform
    instruction-tuned baselines, yet all exhibit distinct vulnerability profiles,
    with misleading suggestions universally effective and social pressure showing
    model-specific efficacy. Through trajectory analysis, we identify five failure
    modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility,
    and Reasoning Fatigue) with the first two accounting for 50% of failures. We further
    demonstrate that Confidence-Aware Response Generation (CARG), effective for standard
    LLMs, fails for reasoning models due to overconfidence induced by extended reasoning
    traces; counterintuitively, random confidence embedding outperforms targeted extraction.
    Our results highlight that reasoning capabilities do not automatically confer
    adversarial robustness and that confidence-based defenses require fundamental
    redesign for reasoning models.'
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13093
  arxiv_url: http://arxiv.org/abs/2602.13093
  key_findings: 'Large reasoning models with reasoning capabilities achieve state-of-the-art
    performance on complex tasks, but their robustness under multi-turn adversarial
    pressure remains underexplored. We evaluate nine frontier reasoning models under
    adversarial attacks. Our findings reveal that reasoning confers meaningful but
    incomplete robustness: most reasoning models studied significantly outperform
    instruction-tuned baselines, yet all exhibit distinct vulnerability profiles,
    with misleading suggestions universally effective and social pressure showing
    model-specific efficacy.'
- id: '2602.13088'
  title: How cyborg propaganda reshapes collective action
  authors: Jonas R. Kunst, Kinga Bierwiaczonek, Meeyoung Cha, Omid V. Ebrahimi, Marc
    Fawcett-Atkinson, Asbjørn Følstad, Anton Gollwitzer, Nils Köbis, Gary Marcus,
    Jon Roozenbeek, Daniel Thilo Schroeder, Jay J. Van Bavel, Sander van der Linden,
    Rory White, Live Leonhardsen Wilhelmsen
  abstract: 'The distinction between genuine grassroots activism and automated influence
    operations is collapsing. While policy debates focus on bot farms, a distinct
    threat to democracy is emerging via partisan coordination apps and artificial
    intelligence-what we term ''cyborg propaganda.'' This architecture combines large
    numbers of verified humans with adaptive algorithmic automation, enabling a closed-loop
    system. AI tools monitor online sentiment to optimize directives and generate
    personalized content for users to post online. Cyborg propaganda thereby exploits
    a critical legal shield: by relying on verified citizens to ratify and disseminate
    messages, these campaigns operate in a regulatory gray zone, evading liability
    frameworks designed for automated botnets. We explore the collective action paradox
    of this technology: does it democratize power by ''unionizing'' influence (pooling
    the reach of dispersed citizens to overcome the algorithmic invisibility of isolated
    voices), or does it reduce citizens to ''cognitive proxies'' of a central directive?
    We argue that cyborg propaganda fundamentally alters the digital public square,
    shifting political discourse from a democratic contest of individual ideas to
    a battle of algorithmic campaigns. We outline a research agenda to distinguish
    organic from coordinated information diffusion and propose governance frameworks
    to address the regulatory challenges of AI-assisted collective expression.'
  published: '2026-02-13'
  pdf_url: http://arxiv.org/pdf/2602.13088
  arxiv_url: http://arxiv.org/abs/2602.13088
  key_findings: The distinction between genuine grassroots activism and automated
    influence operations is collapsing. While policy debates focus on bot farms, a
    distinct threat to democracy is emerging via partisan coordination apps and artificial
    intelligence-what we term 'cyborg propaganda. ' This architecture combines large
    numbers of verified humans with adaptive algorithmic automation, enabling a closed-loop
    system.
