papers:
- id: '2602.17664'
  title: Sink-Aware Pruning for Diffusion Language Models
  authors: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen
  abstract: 'Diffusion Language Models (DLMs) incur high inference cost due to iterative
    denoising, motivating efficient pruning. Existing pruning heuristics largely inherited
    from autoregressive (AR) LLMs, typically preserve attention sink tokens because
    AR sinks serve as stable global anchors. We show that this assumption does not
    hold for DLMs: the attention-sink position exhibits substantially higher variance
    over the full generation trajectory (measured by how the dominant sink locations
    shift across timesteps), indicating that sinks are often transient and less structurally
    essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware
    Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior
    studies usually keep sinks for AR LLMs). Without retraining, our method achieves
    a better quality-efficiency trade-off and outperforms strong prior pruning baselines
    under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17664
  arxiv_url: http://arxiv.org/abs/2602.17664
  key_findings: 'Diffusion Language Models (DLMs) incur high inference cost due to
    iterative denoising, motivating efficient pruning. Existing pruning heuristics
    largely inherited from autoregressive (AR) LLMs, typically preserve attention
    sink tokens because AR sinks serve as stable global anchors. We show that this
    assumption does not hold for DLMs: the attention-sink position exhibits substantially
    higher variance over the full generation trajectory (measured by how the dominant
    sink locations shift across timesteps), indicating that sinks are often transient
    and less structurally essential than in AR models.'
- id: '2602.17663'
  title: 'CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation
    Extraction from Multilingual Historical Texts'
  authors: Juri Opitz, Corina Racl√©, Emanuela Boros, Andrianos Michail, Matteo Romanello,
    Maud Ehrmann, Simon Clematide
  abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation
    extraction from noisy, multilingual historical texts. Building on the HIPE-2020
    and HIPE-2022 campaigns, it extends the series toward semantic relation extraction
    by targeting the task of identifying person--place associations in multiple languages
    and time periods. Systems are asked to classify relations of two types - $at$
    ("Has the person ever been at this place?") and $isAt$ ("Is the person located
    at this place around publication time?") - requiring reasoning over temporal and
    geographical cues. The lab introduces a three-fold evaluation profile that jointly
    assesses accuracy, computational efficiency, and domain generalization. By linking
    relation extraction to large-scale historical data processing, HIPE-2026 aims
    to support downstream applications in knowledge-graph construction, historical
    biography reconstruction, and spatial analysis in digital humanities.
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17663
  arxiv_url: http://arxiv.org/abs/2602.17663
  key_findings: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation
    extraction from noisy, multilingual historical texts. Building on the HIPE-2020
    and HIPE-2022 campaigns, it extends the series toward semantic relation extraction
    by targeting the task of identifying person--place associations in multiple languages
    and time periods. Systems are asked to classify relations of two types - $at$
    ("Has the person ever been at this place.
- id: '2602.17658'
  title: 'MARS: Margin-Aware Reward-Modeling with Self-Refinement'
  authors: Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon
  abstract: Reward modeling is a core component of modern alignment pipelines including
    RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO.
    However, training reliable reward models relies heavily on human-labeled preference
    data, which is costly and limited, motivating the use of data augmentation. Existing
    augmentation approaches typically operate at the representation or semantic level
    and remain agnostic to the reward model's estimation difficulty. In this paper,
    we propose MARS, an adaptive, margin-aware augmentation and sampling strategy
    that explicitly targets ambiguous and failure modes of the reward model. Our proposed
    framework, MARS, concentrates augmentation on low-margin (ambiguous) preference
    pairs where the reward model is most uncertain, and iteratively refines the training
    distribution via hard-sample augmentation. We provide theoretical guarantees showing
    that this strategy increases the average curvature of the loss function hence
    enhance information and improves conditioning, along with empirical results demonstrating
    consistent gains over uniform augmentation for robust reward modeling.
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17658
  arxiv_url: http://arxiv.org/abs/2602.17658
  key_findings: Reward modeling is a core component of modern alignment pipelines
    including RLHF and RLAIF, underpinning policy optimization methods including PPO
    and TRPO. However, training reliable reward models relies heavily on human-labeled
    preference data, which is costly and limited, motivating the use of data augmentation.
    Existing augmentation approaches typically operate at the representation or semantic
    level and remain agnostic to the reward model's estimation difficulty.
- id: '2602.17645'
  title: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting
  authors: Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen
  abstract: 'Black-box adversarial attacks on Large Vision-Language Models (LVLMs)
    are challenging due to missing gradients and complex multimodal boundaries. While
    prior state-of-the-art transfer-based approaches like M-Attack perform well using
    local crop-level matching between source and target images, we find this induces
    high-variance, nearly orthogonal gradients across iterations, violating coherent
    local alignment and destabilizing optimization. We attribute this to (i) ViT translation
    sensitivity that yields spike-like gradients and (ii) structural asymmetry between
    source and target crops. We reformulate local matching as an asymmetric expectation
    over source transformations and target semantics, and build a gradient-denoising
    upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients
    from multiple independently sampled local views per iteration to reduce variance.
    On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target
    augmentation with a small auxiliary set from a semantically correlated distribution,
    producing a smoother, lower-variance target manifold. We further reinterpret momentum
    as Patch Momentum, replaying historical crop gradients; combined with a refined
    patch-size ensemble (PE+), this strengthens transferable directions. Together
    these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that
    substantially improves transfer-based black-box attacks on frontier LVLMs: boosting
    success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and
    GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data
    are publicly available at: https://github.com/vila-lab/M-Attack-V2.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17645
  arxiv_url: http://arxiv.org/abs/2602.17645
  key_findings: Black-box adversarial attacks on Large Vision-Language Models (LVLMs)
    are challenging due to missing gradients and complex multimodal boundaries. While
    prior state-of-the-art transfer-based approaches like M-Attack perform well using
    local crop-level matching between source and target images, we find this induces
    high-variance, nearly orthogonal gradients across iterations, violating coherent
    local alignment and destabilizing optimization. We attribute this to (i) ViT translation
    sensitivity that yields spike-like gradients and (ii) structural asymmetry between
    source and target crops.
- id: '2602.17641'
  title: 'FAMOSE: A ReAct Approach to Automated Feature Discovery'
  authors: Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li
  abstract: Feature engineering remains a critical yet challenging bottleneck in machine
    learning, particularly for tabular data, as identifying optimal features from
    an exponentially large feature space traditionally demands substantial domain
    expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation
    and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm
    to autonomously explore, generate, and refine features while integrating feature
    selection and evaluation tools within an agent architecture. To our knowledge,
    FAMOSE represents the first application of an agentic ReAct framework to automated
    feature engineering, especially for both regression and classification tasks.
    Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art
    on classification tasks (especially tasks with more than 10K instances, where
    ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression
    tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors
    than other algorithms. We hypothesize that FAMOSE's strong performance is because
    ReAct allows the LLM context window to record (via iterative feature discovery
    and evaluation steps) what features did or did not work. This is similar to a
    few-shot prompt and guides the LLM to invent better, more innovative features.
    Our work offers evidence that AI agents are remarkably effective in solving problems
    that require highly inventive solutions, such as feature engineering.
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17641
  arxiv_url: http://arxiv.org/abs/2602.17641
  key_findings: Feature engineering remains a critical yet challenging bottleneck
    in machine learning, particularly for tabular data, as identifying optimal features
    from an exponentially large feature space traditionally demands substantial domain
    expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation
    and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm
    to autonomously explore, generate, and refine features while integrating feature
    selection and evaluation tools within an agent architecture. To our knowledge,
    FAMOSE represents the first application of an agentic ReAct framework to automated
    feature engineering, especially for both regression and classification tasks.
- id: '2602.17634'
  title: 'Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting'
  authors: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim
  abstract: 'Learning time series foundation models has been shown to be a promising
    approach for zero-shot time series forecasting across diverse time series domains.
    Insofar as scaling has been a critical driver of performance of foundation models
    in other modalities such as language and vision, much recent work on time series
    foundation modeling has focused on scaling. This has resulted in time series foundation
    models with hundreds of millions of parameters that are, while performant, inefficient
    and expensive to use in practice. This paper describes a simple recipe for learning
    efficient foundation models for zero-shot time series forecasting that are orders
    of magnitude smaller. We show that large-scale transformers are not necessary:
    small hybrid models that interleave long convolution and linear RNN layers (in
    particular DeltaNet layers) can match the performance of larger transformer-based
    models while being more than a hundred times smaller. We also describe several
    data augmentation and inference strategies that further improve performance. This
    recipe results in Reverso, a family of efficient time series foundation models
    for zero-shot forecasting that significantly push the performance-efficiency Pareto
    frontier.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17634
  arxiv_url: http://arxiv.org/abs/2602.17634
  key_findings: Learning time series foundation models has been shown to be a promising
    approach for zero-shot time series forecasting across diverse time series domains.
    Insofar as scaling has been a critical driver of performance of foundation models
    in other modalities such as language and vision, much recent work on time series
    foundation modeling has focused on scaling. This has resulted in time series foundation
    models with hundreds of millions of parameters that are, while performant, inefficient
    and expensive to use in practice.
- id: '2602.17633'
  title: 'When to Trust the Cheap Check: Weak and Strong Verification for Reasoning'
  authors: Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani
  abstract: 'Reasoning with LLMs increasingly unfolds inside a broader verification
    loop. Internally, systems use cheap checks, such as self-consistency or proxy
    rewards, which we call weak verification. Externally, users inspect outputs and
    steer the model through feedback until results are trustworthy, which we call
    strong verification. These signals differ sharply in cost and reliability: strong
    verification can establish trust but is resource-intensive, while weak verification
    is fast and scalable but noisy and imperfect. We formalize this tension through
    weak--strong verification policies, which decide when to accept or reject based
    on weak verification and when to defer to strong verification. We introduce metrics
    capturing incorrect acceptance, incorrect rejection, and strong-verification frequency.
    Over population, we show that optimal policies admit a two-threshold structure
    and that calibration and sharpness govern the value of weak verifiers. Building
    on this, we develop an online algorithm that provably controls acceptance and
    rejection errors without assumptions on the query stream, the language model,
    or the weak verifier.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17633
  arxiv_url: http://arxiv.org/abs/2602.17633
  key_findings: Reasoning with LLMs increasingly unfolds inside a broader verification
    loop. Internally, systems use cheap checks, such as self-consistency or proxy
    rewards, which we call weak verification. Externally, users inspect outputs and
    steer the model through feedback until results are trustworthy, which we call
    strong verification.
- id: '2602.17632'
  title: 'SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer'
  authors: Nathan S. de Lara, Florian Shkurti
  abstract: Modern offline Reinforcement Learning (RL) methods find performant actor-critics,
    however, fine-tuning these actor-critics online with value-based RL algorithms
    typically causes immediate drops in performance. We provide evidence consistent
    with the hypothesis that, in the loss landscape, offline maxima for prior algorithms
    and online maxima are separated by low-performance valleys that gradient-based
    fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC),
    an offline RL method designed to learn actor-critics that transition to online
    value-based RL algorithms with no drop in performance. SMAC avoids valleys between
    offline and online maxima by regularizing the Q-function during the offline phase
    to respect a first-order derivative equality between the score of the policy and
    action-gradient of the Q-function. We experimentally demonstrate that SMAC converges
    to offline maxima that are connected to better online maxima via paths with monotonically
    increasing reward found by first-order optimization. SMAC achieves smooth transfer
    to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces
    regret by 34-58% over the best baseline.
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17632
  arxiv_url: http://arxiv.org/abs/2602.17632
  key_findings: Modern offline Reinforcement Learning (RL) methods find performant
    actor-critics, however, fine-tuning these actor-critics online with value-based
    RL algorithms typically causes immediate drops in performance. We provide evidence
    consistent with the hypothesis that, in the loss landscape, offline maxima for
    prior algorithms and online maxima are separated by low-performance valleys that
    gradient-based fine-tuning traverses. Following this, we present Score Matched
    Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that
    transition to online value-based RL algorithms with no drop in performance.
- id: '2602.17616'
  title: 'Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs'
  authors: Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han
  abstract: 'Reinforcement learning (RL) is widely used to improve large language
    models on reasoning tasks, and asynchronous RL training is attractive because
    it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient
    methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient
    estimator markedly $\textbf{higher variance}$: training on stale rollouts creates
    heavy-tailed importance ratios, causing a small fraction of samples to dominate
    updates. This amplification makes gradients noisy and learning unstable relative
    to matched on-policy training. Across math and general reasoning benchmarks, we
    find collapse is reliably predicted by effective sample size (ESS) and unstable
    gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled
    $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization
    method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based
    on effective sample size to dampen unreliable updates, and (ii) applies a closed-form
    minimum-variance baseline for the off-policy setting, avoiding an auxiliary value
    model and adding minimal overhead. Empirically, VCPO substantially improves robustness
    for asynchronous training across math, general reasoning, and tool-use tasks,
    outperforming a broad suite of baselines spanning masking/clipping stabilizers
    and algorithmic variants. This reduces long-context, multi-turn training time
    by 2.5$\times$ while matching synchronous performance, demonstrating that explicit
    control of policy-gradient variance is key for reliable asynchronous RL at scale.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17616
  arxiv_url: http://arxiv.org/abs/2602.17616
  key_findings: 'Reinforcement learning (RL) is widely used to improve large language
    models on reasoning tasks, and asynchronous RL training is attractive because
    it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient
    methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient
    estimator markedly $\textbf{higher variance}$: training on stale rollouts creates
    heavy-tailed importance ratios, causing a small fraction of samples to dominate
    updates. This amplification makes gradients noisy and learning unstable relative
    to matched on-policy training.'
- id: '2602.17608'
  title: Towards Anytime-Valid Statistical Watermarking
  authors: Baihe Huang, Eric Xu, Kannan Ramchandran, Jiantao Jiao, Michael I. Jordan
  abstract: 'The proliferation of Large Language Models (LLMs) necessitates efficient
    mechanisms to distinguish machine-generated content from human text. While statistical
    watermarking has emerged as a promising solution, existing methods suffer from
    two critical limitations: the lack of a principled approach for selecting sampling
    distributions and the reliance on fixed-horizon hypothesis testing, which precludes
    valid early stopping. In this paper, we bridge this gap by developing the first
    e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal
    sampling with anytime-valid inference. Unlike traditional approaches where optional
    stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference
    by constructing a test supermartingale for the detection process. By leveraging
    an anchor distribution to approximate the target model, we characterize the optimal
    e-value with respect to the worst-case log-growth rate and derive the optimal
    expected stopping time. Our theoretical claims are substantiated by simulations
    and evaluations on established benchmarks, showing that our framework can significantly
    enhance sample efficiency, reducing the average token budget required for detection
    by 13-15% relative to state-of-the-art baselines.'
  published: '2026-02-19'
  pdf_url: http://arxiv.org/pdf/2602.17608
  arxiv_url: http://arxiv.org/abs/2602.17608
  key_findings: 'The proliferation of Large Language Models (LLMs) necessitates efficient
    mechanisms to distinguish machine-generated content from human text. While statistical
    watermarking has emerged as a promising solution, existing methods suffer from
    two critical limitations: the lack of a principled approach for selecting sampling
    distributions and the reliance on fixed-horizon hypothesis testing, which precludes
    valid early stopping. In this paper, we bridge this gap by developing the first
    e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal
    sampling with anytime-valid inference.'
