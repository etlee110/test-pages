papers:
- id: '2602.11146'
  title: 'Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling'
  authors: Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao,
    Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo
  abstract: Preference optimization for diffusion and flow-matching models relies
    on reward functions that are both discriminatively robust and computationally
    efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider,
    leveraging their rich multimodal priors to guide alignment. However, their computation
    and memory cost can be substantial, and optimizing a latent diffusion generator
    through a pixel-space reward introduces a domain mismatch that complicates alignment.
    In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that
    formulates preference learning directly on noisy diffusion states. Our method
    introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent
    uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a
    timestep-conditioned reward head, and supports inference-time noise ensembling,
    providing a diffusion-native mechanism for test-time scaling and robust rewarding.
    Across image alignment benchmarks, DiNa-LRM substantially outperforms existing
    diffusion-based reward baselines and achieves performance competitive with state-of-the-art
    VLMs at a fraction of the computational cost. In preference optimization, we demonstrate
    that DiNa-LRM improves preference optimization dynamics, enabling faster and more
    resource-efficient model alignment.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11146
  arxiv_url: http://arxiv.org/abs/2602.11146
  key_findings: Preference optimization for diffusion and flow-matching models relies
    on reward functions that are both discriminatively robust and computationally
    efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider,
    leveraging their rich multimodal priors to guide alignment. However, their computation
    and memory cost can be substantial, and optimizing a latent diffusion generator
    through a pixel-space reward introduces a domain mismatch that complicates alignment.
- id: '2602.11144'
  title: 'GENIUS: Generative Fluid Intelligence Evaluation Suite'
  authors: Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui
    Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang
  abstract: 'Unified Multimodal Models (UMMs) have shown remarkable progress in visual
    generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized
    Intelligence}$, which relies on recalling accumulated knowledge and learned schemas.
    This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity
    to induce patterns, reason through constraints, and adapt to novel scenarios on
    the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$
    ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite).
    We formalize $\textit{GFI}$ as a synthesis of three primitives. These include
    $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences),
    $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors),
    and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive
    physics). Collectively, these primitives challenge models to solve problems grounded
    entirely in the immediate context. Our systematic evaluation of 12 representative
    models reveals significant performance deficits in these tasks. Crucially, our
    diagnostic analysis disentangles these failure modes. It demonstrates that deficits
    stem from limited context comprehension rather than insufficient intrinsic generative
    capability. To bridge this gap, we propose a training-free attention intervention
    strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$,
    guiding the field beyond knowledge utilization toward dynamic, general-purpose
    reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.'
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11144
  arxiv_url: http://arxiv.org/abs/2602.11144
  key_findings: 'Unified Multimodal Models (UMMs) have shown remarkable progress in
    visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized
    Intelligence}$, which relies on recalling accumulated knowledge and learned schemas.
    This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity
    to induce patterns, reason through constraints, and adapt to novel scenarios on
    the fly.'
- id: '2602.11142'
  title: Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing
    Flows
  authors: Shaswat Garg, Matin Moezzi, Brandon Da Silva
  abstract: Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides
    a powerful framework for tackling complex, long-horizon tasks by decomposing them
    into structured subgoals. However, its practical adoption is hindered by poor
    data efficiency and limited policy expressivity, especially in offline or data-scarce
    regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning
    (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive
    normalizing flow policies at both the high- and low-levels of the hierarchy is
    introduced. This design enables tractable log-likelihood computation, efficient
    sampling, and the ability to model rich multimodal behaviors. New theoretical
    guarantees are derived, including explicit KL-divergence bounds for Real-valued
    non-volume preserving (RealNVP) policies and PAC-style sample efficiency results,
    showing that NF-HIQL preserves stability while improving generalization. Empirically,
    NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling,
    and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior
    goal-conditioned and hierarchical baselines, demonstrating superior robustness
    under limited data and highlighting the potential of flow-based architectures
    for scalable, data-efficient hierarchical reinforcement learning.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11142
  arxiv_url: http://arxiv.org/abs/2602.11142
  key_findings: Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides
    a powerful framework for tackling complex, long-horizon tasks by decomposing them
    into structured subgoals. However, its practical adoption is hindered by poor
    data efficiency and limited policy expressivity, especially in offline or data-scarce
    regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning
    (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive
    normalizing flow policies at both the high- and low-levels of the hierarchy is
    introduced.
- id: '2602.11137'
  title: Weight Decay Improves Language Model Plasticity
  authors: Tessa Han, Sebastian Bordt, Hanlin Zhang, Sham Kakade
  abstract: The prevailing paradigm in large language model (LLM) development is to
    pretrain a base model, then perform further training to improve performance and
    model behavior. However, hyperparameter optimization and scaling laws have been
    studied primarily from the perspective of the base model's validation loss, ignoring
    downstream adaptability. In this work, we study pretraining from the perspective
    of model plasticity, that is, the ability of the base model to successfully adapt
    to downstream tasks through fine-tuning. We focus on the role of weight decay,
    a key regularization parameter during pretraining. Through systematic experiments,
    we show that models trained with larger weight decay values are more plastic,
    meaning they show larger performance gains when fine-tuned on downstream tasks.
    This phenomenon can lead to counterintuitive trade-offs where base models that
    perform worse after pretraining can perform better after fine-tuning. Further
    investigation of weight decay's mechanistic effects on model behavior reveals
    that it encourages linearly separable representations, regularizes attention matrices,
    and reduces overfitting on the training data. In conclusion, this work demonstrates
    the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter
    optimization and casts light on the multifaceted role of that a single optimization
    hyperparameter plays in shaping model behavior.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11137
  arxiv_url: http://arxiv.org/abs/2602.11137
  key_findings: The prevailing paradigm in large language model (LLM) development
    is to pretrain a base model, then perform further training to improve performance
    and model behavior. However, hyperparameter optimization and scaling laws have
    been studied primarily from the perspective of the base model's validation loss,
    ignoring downstream adaptability. In this work, we study pretraining from the
    perspective of model plasticity, that is, the ability of the base model to successfully
    adapt to downstream tasks through fine-tuning.
- id: '2602.11136'
  title: 'FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight'
  authors: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu
  abstract: 'As LLM-based agents increasingly operate in high-stakes domains with
    real-world consequences, ensuring their behavioral safety becomes paramount. The
    dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how
    can probabilistic systems reliably supervise other probabilistic systems without
    inheriting their failure modes? We argue that formal verification offers a principled
    escape from this dilemma, yet its adoption has been hindered by a critical bottleneck:
    the translation from natural language requirements to formal specifications. This
    paper bridges this gap by proposing , a neuro-symbolic framework that employs
    a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers
    that top-down decompose high-level human intent into atomic, verifiable constraints,
    then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability
    modulo theories solving, which produces mathematical guarantees rather than probabilistic
    scores. We validate across three benchmarks spanning behavioral safety, multi-domain
    constraint adherence, and agentic upward deception detection. Experiments on 7
    agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge
    baselines, enables weak-to-strong generalization where a 7B judge achieves over
    90% accuracy detecting deception from 72B agents, and provides near-linear safety
    improvement through iterative refinement.'
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11136
  arxiv_url: http://arxiv.org/abs/2602.11136
  key_findings: 'As LLM-based agents increasingly operate in high-stakes domains with
    real-world consequences, ensuring their behavioral safety becomes paramount. The
    dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how
    can probabilistic systems reliably supervise other probabilistic systems without
    inheriting their failure modes. We argue that formal verification offers a principled
    escape from this dilemma, yet its adoption has been hindered by a critical bottleneck:
    the translation from natural language requirements to formal specifications.'
- id: '2602.11114'
  title: Learning to Compose for Cross-domain Agentic Workflow Generation
  authors: Jialiang Wang, Shengxiang Xu, Hanmo Liu, Jiachuan Wang, Yuyu Luo, Shimin
    Di, Min-Ling Zhang, Lei Chen
  abstract: Automatically generating agentic workflows -- executable operator graphs
    or codes that orchestrate reasoning, verification, and repair -- has become a
    practical way to solve complex tasks beyond what single-pass LLM generation can
    reliably handle. Yet what constitutes a good workflow depends heavily on the task
    distribution and the available operators. Under domain shift, current systems
    typically rely on iterative workflow refinement to discover a feasible workflow
    from a large workflow space, incurring high iteration costs and yielding unstable,
    domain-specific behavior. In response, we internalize a decompose-recompose-decide
    mechanism into an open-source LLM for cross-domain workflow generation. To decompose,
    we learn a compact set of reusable workflow capabilities across diverse domains.
    To recompose, we map each input task to a sparse composition over these bases
    to generate a task-specific workflow in a single pass. To decide, we attribute
    the success or failure of workflow generation to counterfactual contributions
    from learned capabilities, thereby capturing which capabilities actually drive
    success by their marginal effects. Across stringent multi-domain, cross-domain,
    and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement
    baselines that consume 20 iterations, while substantially reducing generation
    latency and cost.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11114
  arxiv_url: http://arxiv.org/abs/2602.11114
  key_findings: Automatically generating agentic workflows -- executable operator
    graphs or codes that orchestrate reasoning, verification, and repair -- has become
    a practical way to solve complex tasks beyond what single-pass LLM generation
    can reliably handle. Yet what constitutes a good workflow depends heavily on the
    task distribution and the available operators. Under domain shift, current systems
    typically rely on iterative workflow refinement to discover a feasible workflow
    from a large workflow space, incurring high iteration costs and yielding unstable,
    domain-specific behavior.
- id: '2602.11103'
  title: 'GameDevBench: Evaluating Agentic Capabilities Through Game Development'
  authors: Wayne Chi, Yixiong Fang, Arnav Yayavaram, Siddharth Yayavaram, Seth Karten,
    Qiuhong Anna Wei, Runkun Chen, Alexander Wang, Valerie Chen, Ameet Talwalkar,
    Chris Donahue
  abstract: Despite rapid progress on coding agents, progress on their multimodal
    counterparts has lagged behind. A key challenge is the scarcity of evaluation
    testbeds that combine the complexity of software development with the need for
    deep multimodal understanding. Game development provides such a testbed as agents
    must navigate large, dense codebases while manipulating intrinsically multimodal
    assets such as shaders, sprites, and animations within a visual game scene. We
    present GameDevBench, the first benchmark for evaluating agents on game development
    tasks. GameDevBench consists of 132 tasks derived from web and video tutorials.
    Tasks require significant multimodal understanding and are complex -- the average
    solution requires over three times the amount of lines of code and file changes
    compared to prior software development benchmarks. Agents still struggle with
    game development, with the best agent solving only 54.5% of tasks. We find a strong
    correlation between perceived task difficulty and multimodal complexity, with
    success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics
    tasks. To improve multimodal capability, we introduce two simple image and video-based
    feedback mechanisms for agents. Despite their simplicity, these methods consistently
    improve performance, with the largest change being an increase in Claude Sonnet
    4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support
    further research into agentic game development.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11103
  arxiv_url: http://arxiv.org/abs/2602.11103
  key_findings: Despite rapid progress on coding agents, progress on their multimodal
    counterparts has lagged behind. A key challenge is the scarcity of evaluation
    testbeds that combine the complexity of software development with the need for
    deep multimodal understanding. Game development provides such a testbed as agents
    must navigate large, dense codebases while manipulating intrinsically multimodal
    assets such as shaders, sprites, and animations within a visual game scene.
- id: '2602.11096'
  title: Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away
  authors: Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Furong Huang,
    Dinesh Manocha, Amrit Singh Bedi
  abstract: 'Reinforcement learning (RL) based post-training for explicit chain-of-thought
    (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning
    models (MLRMs). But recent evidence shows that it can simultaneously degrade safety
    alignment and increase jailbreak success rates. We propose SafeThink, a lightweight
    inference-time defense that treats safety recovery as a satisficing constraint
    rather than a maximization objective. SafeThink monitors the evolving reasoning
    trace with a safety reward model and conditionally injects an optimized short
    corrective prefix ("Wait, think safely") only when the safety threshold is violated.
    In our evaluations across six open-source MLRMs and four jailbreak benchmarks
    (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack
    success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision:
    69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy:
    65.20% to 65.00%). A key empirical finding from our experiments is that safety
    recovery is often only a few steering steps away: intervening in the first 1-3
    reasoning steps typically suffices to redirect the full generation toward safe
    completions.'
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11096
  arxiv_url: http://arxiv.org/abs/2602.11096
  key_findings: Reinforcement learning (RL) based post-training for explicit chain-of-thought
    (e. g. , GRPO) improves the reasoning ability of multimodal large-scale reasoning
    models (MLRMs).
- id: '2602.11090'
  title: Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates
  authors: Carlos Stein Brito
  abstract: Neural PDE surrogates are often deployed in data-limited or partially
    observed regimes where downstream decisions depend on calibrated uncertainty in
    addition to low prediction error. Existing approaches obtain uncertainty through
    ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration.
    Cross-regularized uncertainty learns uncertainty parameters during training using
    gradients routed through a held-out regularization split. The predictor is optimized
    on the training split for fit, while low-dimensional uncertainty controls are
    optimized on the regularization split to reduce train-test mismatch, yielding
    regime-adaptive uncertainty without per-regime noise tuning. The framework can
    learn continuous noise levels at the output head, within hidden features, or within
    operator-specific components such as spectral modes. We instantiate the approach
    in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction
    and training-set size. Across these sweeps, the learned predictive distributions
    are better calibrated on held-out splits and the resulting uncertainty fields
    concentrate in high-error regions in one-step spatial diagnostics.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11090
  arxiv_url: http://arxiv.org/abs/2602.11090
  key_findings: Neural PDE surrogates are often deployed in data-limited or partially
    observed regimes where downstream decisions depend on calibrated uncertainty in
    addition to low prediction error. Existing approaches obtain uncertainty through
    ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration.
    Cross-regularized uncertainty learns uncertainty parameters during training using
    gradients routed through a held-out regularization split.
- id: '2602.11089'
  title: 'DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement
    Learning'
  authors: Yicheng Chen, Zerun Ma, Xinchen Xie, Yining Li, Kai Chen
  abstract: In the current landscape of Large Language Models (LLMs), the curation
    of large-scale, high-quality training data is a primary driver of model performance.
    A key lever is the \emph{data recipe}, which comprises a data processing pipeline
    to transform raw sources into training corpora. Despite the growing use of LLMs
    to automate individual data processing steps, such as data synthesis and filtering,
    the overall design of data recipes remains largely manual and labor-intensive,
    requiring substantial human expertise and iteration. To bridge this gap, we formulate
    \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark
    and a pool of available data sources, a model is required to output a complete
    data recipe that adapts a base LLM to the target task. We present DataChef-32B,
    which performs online reinforcement learning using a proxy reward that predicts
    downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B
    produces practical recipes that reach comparable downstream performance to those
    curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base
    to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This
    work sheds new light on automating LLM training and developing self-evolving AI
    systems.
  published: '2026-02-11'
  pdf_url: http://arxiv.org/pdf/2602.11089
  arxiv_url: http://arxiv.org/abs/2602.11089
  key_findings: In the current landscape of Large Language Models (LLMs), the curation
    of large-scale, high-quality training data is a primary driver of model performance.
    A key lever is the \emph{data recipe}, which comprises a data processing pipeline
    to transform raw sources into training corpora. Despite the growing use of LLMs
    to automate individual data processing steps, such as data synthesis and filtering,
    the overall design of data recipes remains largely manual and labor-intensive,
    requiring substantial human expertise and iteration.
