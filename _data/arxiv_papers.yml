papers:
- id: '2602.12125'
  title: 'Learning beyond Teacher: Generalized On-Policy Distillation with Reward
    Extrapolation'
  authors: Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin
  abstract: 'On-policy distillation (OPD), which aligns the student with the teacher''s
    logit distribution on student-generated trajectories, has demonstrated strong
    empirical gains in improving student performance and often outperforms off-policy
    distillation and reinforcement learning (RL) paradigms. In this work, we first
    theoretically show that OPD is a special case of dense KL-constrained RL where
    the reward function and the KL regularization are always weighted equally and
    the reference model can by any model. Then, we propose the Generalized On-Policy
    Distillation (G-OPD) framework, which extends the standard OPD objective by introducing
    a flexible reference model and a reward scaling factor that controls the relative
    weight of the reward term against the KL regularization. Through comprehensive
    experiments on math reasoning and code generation tasks, we derive two novel insights:
    (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation),
    which we term ExOPD, consistently improves over standard OPD across a range of
    teacher-student size pairings. In particular, in the setting where we merge the
    knowledge from different domain experts, obtained by applying domain-specific
    RL to the same student model, back into the original student, ExOPD enables the
    student to even surpass the teacher''s performance boundary and outperform the
    domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak
    distillation setting (i.e., distilling a smaller student from a larger teacher),
    performing reward correction by choosing the reference model as the teacher''s
    base model before RL yields a more accurate reward signal and further improves
    distillation performance. However, this choice assumes access to the teacher''s
    pre-RL variant and incurs more computational overhead. We hope our work offers
    new insights for future research on OPD.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12125
  arxiv_url: http://arxiv.org/abs/2602.12125
  key_findings: On-policy distillation (OPD), which aligns the student with the teacher's
    logit distribution on student-generated trajectories, has demonstrated strong
    empirical gains in improving student performance and often outperforms off-policy
    distillation and reinforcement learning (RL) paradigms. In this work, we first
    theoretically show that OPD is a special case of dense KL-constrained RL where
    the reward function and the KL regularization are always weighted equally and
    the reference model can by any model. Then, we propose the Generalized On-Policy
    Distillation (G-OPD) framework, which extends the standard OPD objective by introducing
    a flexible reference model and a reward scaling factor that controls the relative
    weight of the reward term against the KL regularization.
- id: '2602.12123'
  title: 'Meta-Sel: Efficient Demonstration Selection for In-Context Learning via
    Supervised Meta-Learning'
  authors: Xubin Wang, Weijia Jia
  abstract: 'Demonstration selection is a practical bottleneck in in-context learning
    (ICL): under a tight prompt budget, accuracy can change substantially depending
    on which few-shot examples are included, yet selection must remain cheap enough
    to run per query over large candidate pools. We propose Meta-Sel, a lightweight
    supervised meta-learning approach for intent classification that learns a fast,
    interpretable scoring function for (candidate, query) pairs from labeled training
    data. Meta-Sel constructs a meta-dataset by sampling pairs from the training split
    and using class agreement as supervision, then trains a calibrated logistic regressor
    on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility
    ratio. At inference time, the selector performs a single vectorized scoring pass
    over the full candidate pool and returns the top-k demonstrations, requiring no
    model fine-tuning, no online exploration, and no additional LLM calls. This yields
    deterministic rankings and makes the selection mechanism straightforward to audit
    via interpretable feature weights. Beyond proposing Meta-Sel, we provide a broad
    empirical study of demonstration selection, benchmarking 12 methods -- spanning
    prompt engineering baselines, heuristic selection, reinforcement learning, and
    influence-based approaches -- across four intent datasets and five open-source
    LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing
    methods, is particularly effective for smaller models where selection quality
    can partially compensate for limited model capacity, and maintains competitive
    selection-time overhead.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12123
  arxiv_url: http://arxiv.org/abs/2602.12123
  key_findings: 'Demonstration selection is a practical bottleneck in in-context learning
    (ICL): under a tight prompt budget, accuracy can change substantially depending
    on which few-shot examples are included, yet selection must remain cheap enough
    to run per query over large candidate pools. We propose Meta-Sel, a lightweight
    supervised meta-learning approach for intent classification that learns a fast,
    interpretable scoring function for (candidate, query) pairs from labeled training
    data. Meta-Sel constructs a meta-dataset by sampling pairs from the training split
    and using class agreement as supervision, then trains a calibrated logistic regressor
    on two inexpensive meta-features: TF--IDF cosine similarity and a length-compatibility
    ratio.'
- id: '2602.12120'
  title: Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series
    Foundation Models
  authors: Jittarin Jetwiriyanon, Teo Susnjak, Surangika Ranathunga
  abstract: Many universities face increasing financial pressure and rely on accurate
    forecasts of commencing enrolments. However, enrolment forecasting in higher education
    is often data-sparse; annual series are short and affected by reporting changes
    and regime shifts. Popular classical approaches can be unreliable, as parameter
    estimation and model selection are unstable with short samples, and structural
    breaks degrade extrapolation. Recently, TSFMs have provided zero-shot priors,
    delivering strong gains in annual, data-sparse institutional forecasting under
    leakage-disciplined covariate construction. We benchmark multiple TSFM families
    in a zero-shot setting and test a compact, leakage-safe covariate set and introduce
    the Institutional Operating Conditions Index (IOCI), a transferable 0-100 regime
    covariate derived from time-stamped documentary evidence available at each forecast
    origin, alongside Google Trends demand proxies with stabilising feature engineering.
    Using an expanding-window backtest with strict vintage alignment, covariate-conditioned
    TSFMs perform on par with classical benchmarks without institution-specific training,
    with performance differences varying by cohort and model.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12120
  arxiv_url: http://arxiv.org/abs/2602.12120
  key_findings: Many universities face increasing financial pressure and rely on accurate
    forecasts of commencing enrolments. However, enrolment forecasting in higher education
    is often data-sparse; annual series are short and affected by reporting changes
    and regime shifts. Popular classical approaches can be unreliable, as parameter
    estimation and model selection are unstable with short samples, and structural
    breaks degrade extrapolation.
- id: '2602.12117'
  title: 'KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone
    Estimation on Meteorological Satellite'
  authors: Jiakang Shen, Qinghui Chen, Runtong Wang, Chenrui Xu, Jinglin Zhang, Cong
    Bai, Feng Zhang
  abstract: Tropical cyclones (TC) are among the most destructive natural disasters,
    causing catastrophic damage to coastal regions through extreme winds, heavy rainfall,
    and storm surges. Timely monitoring of tropical cyclones is crucial for reducing
    loss of life and property, yet it is hindered by the computational inefficiency
    and high parameter counts of existing methods on resource-constrained edge devices.
    Current physics-guided models suffer from linear feature interactions that fail
    to capture high-order polynomial relationships between TC attributes, leading
    to inflated model sizes and hardware incompatibility. To overcome these challenges,
    this study introduces the Kolmogorov-Arnold Network-based Feature Interaction
    Framework (KAN-FIF), a lightweight multimodal architecture that integrates MLP
    and CNN layers with spline-parameterized KAN layers. For Maximum Sustained Wind
    (MSW) prediction, experiments demonstrate that the KAN-FIF framework achieves
    a $94.8\%$ reduction in parameters (0.99MB vs 19MB) and $68.7\%$ faster inference
    per sample (2.3ms vs 7.35ms) compared to baseline model Phy-CoCo, while maintaining
    superior accuracy with $32.5\%$ lower MAE. The offline deployment experiment of
    the FY-4 series meteorological satellite processor on the Qingyun-1000 development
    board achieved a 14.41ms per-sample inference latency with the KAN-FIF framework,
    demonstrating promising feasibility for operational TC monitoring and extending
    deployability to edge-device AI applications. The code is released at https://github.com/Jinglin-Zhang/KAN-FIF.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12117
  arxiv_url: http://arxiv.org/abs/2602.12117
  key_findings: Tropical cyclones (TC) are among the most destructive natural disasters,
    causing catastrophic damage to coastal regions through extreme winds, heavy rainfall,
    and storm surges. Timely monitoring of tropical cyclones is crucial for reducing
    loss of life and property, yet it is hindered by the computational inefficiency
    and high parameter counts of existing methods on resource-constrained edge devices.
    Current physics-guided models suffer from linear feature interactions that fail
    to capture high-order polynomial relationships between TC attributes, leading
    to inflated model sizes and hardware incompatibility.
- id: '2602.12113'
  title: 'Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with
    Adaptive Reflection and Length Coordinated Penalty'
  authors: Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo, Haobo Wang, Junbo
    Zhao
  abstract: 'Large Reasoning Models (LRMs) have demonstrated remarkable performance
    on complex reasoning tasks by employing test-time scaling. However, they often
    generate over-long chains-of-thought that, driven by substantial reflections such
    as repetitive self-questioning and circular reasoning, lead to high token consumption,
    substantial computational overhead, and increased latency without improving accuracy,
    particularly in smaller models. Our observation reveals that increasing problem
    complexity induces more excessive and unnecessary reflection, which in turn reduces
    accuracy and increases token overhead. To address this challenge, we propose Adaptive
    Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning
    framework designed to dynamically balance reasoning efficiency and solution accuracy.
    ARLCP introduces two key innovations: (1) a reflection penalty that adaptively
    curtails unnecessary reflective steps while preserving essential reasoning, and
    (2) a length penalty calibrated to the estimated complexity of the problem. By
    coordinating these penalties, ARLCP encourages the model to generate more concise
    and effective reasoning paths. We evaluate our method on five mathematical reasoning
    benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B
    models. Experimental results show that ARLCP achieves a superior efficiency-accuracy
    trade-off compared to existing approaches. For the 1.5B model, it reduces the
    average response length by 53.1% while simultaneously improving accuracy by 5.8%.
    For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy
    gain. The code is released at https://github.com/ZeweiYu1/ARLCP .'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12113
  arxiv_url: http://arxiv.org/abs/2602.12113
  key_findings: Large Reasoning Models (LRMs) have demonstrated remarkable performance
    on complex reasoning tasks by employing test-time scaling. However, they often
    generate over-long chains-of-thought that, driven by substantial reflections such
    as repetitive self-questioning and circular reasoning, lead to high token consumption,
    substantial computational overhead, and increased latency without improving accuracy,
    particularly in smaller models. Our observation reveals that increasing problem
    complexity induces more excessive and unnecessary reflection, which in turn reduces
    accuracy and increases token overhead.
- id: '2602.12108'
  title: 'The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context'
  authors: Xiaoyuan Liu, Tian Liang, Dongyang Ma, Deyu Zhou, Haitao Mi, Pinjia He,
    Yan Wang
  abstract: 'In the world of Harry Potter, when Dumbledore''s mind is overburdened,
    he extracts memories into a Pensieve to be revisited later. In the world of AI,
    while we possess the Pensieve-mature databases and retrieval systems, our models
    inexplicably lack the "wand" to operate it. They remain like a Dumbledore without
    agency, passively accepting a manually engineered context as their entire memory.
    This work finally places the wand in the model''s hand. We introduce StateLM,
    a new class of foundation models endowed with an internal reasoning loop to manage
    their own state. We equip our model with a suite of memory tools, such as context
    pruning, document indexing, and note-taking, and train it to actively manage these
    tools. By learning to dynamically engineering its own context, our model breaks
    free from the architectural prison of a fixed window. Experiments across various
    model sizes demonstrate StateLM''s effectiveness across diverse scenarios. On
    long-document QA tasks, StateLMs consistently outperform standard LLMs across
    all model scales; on the chat memory task, they achieve absolute accuracy improvements
    of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the
    performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy,
    whereas standard LLM counterparts struggle around 5%. Ultimately, our approach
    shifts LLMs from passive predictors to state-aware agents where reasoning becomes
    a stateful and manageable process.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12108
  arxiv_url: http://arxiv.org/abs/2602.12108
  key_findings: In the world of Harry Potter, when Dumbledore's mind is overburdened,
    he extracts memories into a Pensieve to be revisited later. In the world of AI,
    while we possess the Pensieve-mature databases and retrieval systems, our models
    inexplicably lack the "wand" to operate it. They remain like a Dumbledore without
    agency, passively accepting a manually engineered context as their entire memory.
- id: '2602.12107'
  title: On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation
    and Partial Coverage
  authors: Haolin Liu, Braham Snyder, Chen-Yu Wei
  abstract: 'We study offline reinforcement learning under $Q^\star$-approximation
    and partial coverage, a setting that motivates practical algorithms such as Conservative
    $Q$-Learning (CQL; Kumar et al., 2020) but has received limited theoretical attention.
    Our work is inspired by the following open question: "Are $Q^\star$-realizability
    and Bellman completeness sufficient for sample-efficient offline RL under partial
    coverage?" We answer in the negative by establishing an information-theoretic
    lower bound. Going substantially beyond this, we introduce a general framework
    that characterizes the intrinsic complexity of a given $Q^\star$ function class,
    inspired by model-free decision-estimation coefficients (DEC) for online RL (Foster
    et al., 2023b; Liu et al., 2025b). This complexity recovers and improves the quantities
    underlying the guarantees of Chen and Jiang (2022) and Uehara et al. (2023), and
    extends to broader settings. Our decision-estimation decomposition can be combined
    with a wide range of $Q^\star$ estimation procedures, modularizing and generalizing
    existing approaches. Beyond the general framework, we make further contributions:
    By developing a novel second-order performance difference lemma, we obtain the
    first $ε^{-2}$ sample complexity under partial coverage for soft $Q$-learning,
    improving the $ε^{-4}$ bound of Uehara et al. (2023). We remove Chen and Jiang''s
    (2022) need for additional online interaction when the value gap of $Q^\star$
    is unknown. We also give the first characterization of offline learnability for
    general low-Bellman-rank MDPs without Bellman completeness (Jiang et al., 2017;
    Du et al., 2021; Jin et al., 2021), a canonical setting in online RL that remains
    unexplored in offline RL except for special cases. Finally, we provide the first
    analysis for CQL under $Q^\star$-realizability and Bellman completeness beyond
    the tabular case.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12107
  arxiv_url: http://arxiv.org/abs/2602.12107
  key_findings: 'We study offline reinforcement learning under $Q^\star$-approximation
    and partial coverage, a setting that motivates practical algorithms such as Conservative
    $Q$-Learning (CQL; Kumar et al. , 2020) but has received limited theoretical attention.
    Our work is inspired by the following open question: "Are $Q^\star$-realizability
    and Bellman completeness sufficient for sample-efficient offline RL under partial
    coverage.'
- id: '2602.12096'
  title: Multi Graph Search for High-Dimensional Robot Motion Planning
  authors: Itamar Mishani, Maxim Likhachev
  abstract: Efficient motion planning for high-dimensional robotic systems, such as
    manipulators and mobile manipulators, is critical for real-time operation and
    reliable deployment. Although advances in planning algorithms have enhanced scalability
    to high-dimensional state spaces, these improvements often come at the cost of
    generating unpredictable, inconsistent motions or requiring excessive computational
    resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based
    motion planning algorithm that generalizes classical unidirectional and bidirectional
    search to a multi-graph setting. MGS maintains and incrementally expands multiple
    implicit graphs over the state space, focusing exploration on high-potential regions
    while allowing initially disconnected subgraphs to be merged through feasible
    transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal,
    and empirically demonstrate its effectiveness on a range of manipulation and mobile
    manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12096
  arxiv_url: http://arxiv.org/abs/2602.12096
  key_findings: Efficient motion planning for high-dimensional robotic systems, such
    as manipulators and mobile manipulators, is critical for real-time operation and
    reliable deployment. Although advances in planning algorithms have enhanced scalability
    to high-dimensional state spaces, these improvements often come at the cost of
    generating unpredictable, inconsistent motions or requiring excessive computational
    resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based
    motion planning algorithm that generalizes classical unidirectional and bidirectional
    search to a multi-graph setting.
- id: '2602.12092'
  title: 'DeepSight: An All-in-One LM Safety Toolkit'
  authors: Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen,
    Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong
    Guo, Tianyi Du, Jingyi Yang, Xuhao Hu, Ziqi Miao, Xiaoya Lu, Jing Shao, Xia Hu
  abstract: As the development of Large Models (LMs) progresses rapidly, their safety
    is also a priority. In current Large Language Models (LLMs) and Multimodal Large
    Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment
    are often handled by separate tools. Specifically, safety evaluation can only
    locate external behavioral risks but cannot figure out internal root causes. Meanwhile,
    safety diagnosis often drifts from concrete risk scenarios and remains at the
    explainable level. In this way, safety alignment lack dedicated explanations of
    changes in internal mechanisms, potentially degrading general capabilities. To
    systematically address these issues, we propose an open-source project, namely
    DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm.
    DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale
    model safety evaluation project consisting of a evaluation toolkit DeepSafe and
    a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a
    connection between the two stages and transform safety evaluation from black-box
    to white-box insight. Besides, DeepSight is the first open source toolkit that
    support the frontier AI risk evaluation and joint safety evaluation and diagnosis.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12092
  arxiv_url: http://arxiv.org/abs/2602.12092
  key_findings: As the development of Large Models (LMs) progresses rapidly, their
    safety is also a priority. In current Large Language Models (LLMs) and Multimodal
    Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment
    are often handled by separate tools. Specifically, safety evaluation can only
    locate external behavioral risks but cannot figure out internal root causes.
- id: '2602.12089'
  title: 'Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates
    in Multi-Party Negotiation'
  authors: Kehang Zhu, Lithium Thain, Vivian Tsai, James Wexler, Crystal Qian
  abstract: 'As AI usage becomes more prevalent in social contexts, understanding
    agent-user interaction is critical to designing systems that improve both individual
    and group outcomes. We present an online behavioral experiment (N = 243) in which
    participants play three multi-turn bargaining games in groups of three. Each game,
    presented in randomized order, grants \textit{access to} a single LLM assistance
    modality: proactive recommendations from an \textit{Advisor}, reactive feedback
    from a \textit{Coach}, or autonomous execution by a \textit{Delegate}; all modalities
    are powered by an underlying LLM that achieves superhuman performance in an all-agent
    environment. On each turn, participants privately decide whether to act manually
    or use the AI modality available in that game. Despite preferring the \textit{Advisor}
    modality, participants achieve the highest mean individual gains with the \textit{Delegate},
    demonstrating a preference-performance misalignment. Moreover, delegation generates
    positive externalities; even non-adopting users in \textit{access-to-delegate}
    treatment groups benefit by receiving higher-quality offers. Mechanism analysis
    reveals that the \textit{Delegate} agent acts as a market maker, injecting rational,
    Pareto-improving proposals that restructure the trading environment. Our research
    reveals a gap between agent capabilities and realized group welfare. While autonomous
    agents can exhibit super-human strategic performance, their impact on realized
    welfare gains can be constrained by interfaces, user perceptions, and adoption
    barriers. Assistance modalities should be designed as mechanisms with endogenous
    participation; adoption-compatible interaction rules are a prerequisite to improving
    human welfare with automated assistance.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12089
  arxiv_url: http://arxiv.org/abs/2602.12089
  key_findings: 'As AI usage becomes more prevalent in social contexts, understanding
    agent-user interaction is critical to designing systems that improve both individual
    and group outcomes. We present an online behavioral experiment (N = 243) in which
    participants play three multi-turn bargaining games in groups of three. Each game,
    presented in randomized order, grants \textit{access to} a single LLM assistance
    modality: proactive recommendations from an \textit{Advisor}, reactive feedback
    from a \textit{Coach}, or autonomous execution by a \textit{Delegate}; all modalities
    are powered by an underlying LLM that achieves superhuman performance in an all-agent
    environment.'
