papers:
- id: '2602.12281'
  title: Scaling Verification Can Be More Effective than Scaling Policy Learning for
    Vision-Language-Action Alignment
  authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea
    Finn, Marco Pavone
  abstract: The long-standing vision of general-purpose robots hinges on their ability
    to understand and act upon natural language instructions. Vision-Language-Action
    (VLA) models have made remarkable progress toward this goal, yet their generated
    actions can still misalign with the given instructions. In this paper, we investigate
    test-time verification as a means to shrink the "intention-action gap.'' We first
    characterize the test-time scaling law for embodied instruction following and
    demonstrate that jointly scaling the number of rephrased instructions and generated
    actions greatly increases test-time sample diversity, often recovering correct
    actions more efficiently than scaling each dimension independently. To capitalize
    on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action
    alignment, and show that our architecture scales gracefully with additional computational
    resources and data. We then introduce "boot-time compute" and a hierarchical verification
    inference pipeline for VLAs. At deployment, our framework precomputes a diverse
    set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates
    action candidates for each instruction, and then uses a verifier to select the
    optimal high-level prompt and low-level action chunks. Compared to scaling policy
    pre-training on the same data, our verification approach yields 22% gains in-distribution
    and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement
    in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains
    in task progress and 9% in success rate.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12281
  arxiv_url: http://arxiv.org/abs/2602.12281
  key_findings: The long-standing vision of general-purpose robots hinges on their
    ability to understand and act upon natural language instructions. Vision-Language-Action
    (VLA) models have made remarkable progress toward this goal, yet their generated
    actions can still misalign with the given instructions. In this paper, we investigate
    test-time verification as a means to shrink the "intention-action gap.
- id: '2602.12279'
  title: 'UniT: Unified Multimodal Chain-of-Thought Test-time Scaling'
  authors: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang
    Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang,
    Serena Yeung-Levy, Felix Juefei-Xu
  abstract: 'Unified models can handle both multimodal understanding and generation
    within a single architecture, yet they typically operate in a single pass without
    iteratively refining their outputs. Many multimodal tasks, especially those involving
    complex spatial compositions, multiple interacting objects, or evolving instructions,
    require decomposing instructions, verifying intermediate results, and making iterative
    corrections. While test-time scaling (TTS) has demonstrated that allocating additional
    inference compute for iterative reasoning substantially improves language model
    performance, extending this paradigm to unified multimodal models remains an open
    challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time
    scaling that enables a single unified model to reason, verify, and refine across
    multiple rounds. UniT combines agentic data synthesis, unified model training,
    and flexible test-time inference to elicit cognitive behaviors including verification,
    subgoal decomposition, and content memory. Our key findings are: (1) unified models
    trained on short reasoning trajectories generalize to longer inference chains
    at test time; (2) sequential chain-of-thought reasoning provides a more scalable
    and compute-efficient TTS strategy than parallel sampling; (3) training on generation
    and editing trajectories improves out-of-distribution visual reasoning. These
    results establish multimodal test-time scaling as an effective paradigm for advancing
    both generation and understanding in unified models.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12279
  arxiv_url: http://arxiv.org/abs/2602.12279
  key_findings: Unified models can handle both multimodal understanding and generation
    within a single architecture, yet they typically operate in a single pass without
    iteratively refining their outputs. Many multimodal tasks, especially those involving
    complex spatial compositions, multiple interacting objects, or evolving instructions,
    require decomposing instructions, verifying intermediate results, and making iterative
    corrections. While test-time scaling (TTS) has demonstrated that allocating additional
    inference compute for iterative reasoning substantially improves language model
    performance, extending this paradigm to unified multimodal models remains an open
    challenge.
- id: '2602.12278'
  title: 'AttentionRetriever: Attention Layers are Secretly Long Document Retrievers'
  authors: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
  abstract: Retrieval augmented generation (RAG) has been widely adopted to help Large
    Language Models (LLMs) to process tasks involving long documents. However, existing
    retrieval models are not designed for long document retrieval and fail to address
    several key challenges of long document retrieval, including context-awareness,
    causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever,
    a novel long document retrieval model that leverages attention mechanism and entity-based
    retrieval to build context-aware embeddings for long document and determine the
    scope of retrieval. With extensive experiments, we found AttentionRetriever is
    able to outperform existing retrieval models on long document retrieval datasets
    by a large margin while remaining as efficient as dense retrieval models.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12278
  arxiv_url: http://arxiv.org/abs/2602.12278
  key_findings: Retrieval augmented generation (RAG) has been widely adopted to help
    Large Language Models (LLMs) to process tasks involving long documents. However,
    existing retrieval models are not designed for long document retrieval and fail
    to address several key challenges of long document retrieval, including context-awareness,
    causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever,
    a novel long document retrieval model that leverages attention mechanism and entity-based
    retrieval to build context-aware embeddings for long document and determine the
    scope of retrieval.
- id: '2602.12276'
  title: Agentic Test-Time Scaling for WebAgents
  authors: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai,
    Michael W. Mahoney, Kurt Keutzer, Amir Gholami
  abstract: 'Test-time scaling has become a standard way to improve performance and
    boost reliability of neural network models. However, its behavior on agentic,
    multi-step tasks remains less well-understood: small per-step errors can compound
    over long horizons; and we find that naive policies that uniformly increase sampling
    show diminishing returns. In this work, we present CATTS, a simple technique for
    dynamically allocating compute for multi-step agents. We first conduct an empirical
    study of inference-time scaling for web agents. We find that uniformly increasing
    per-step compute quickly saturates in long-horizon environments. We then investigate
    stronger aggregation strategies, including an LLM-based Arbiter that can outperform
    naive voting, but that can overrule high-consensus decisions. We show that uncertainty
    statistics derived from the agent''s own vote distribution (entropy and top-1/top-2
    margin) correlate with downstream success and provide a practical signal for dynamic
    compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time
    Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only
    when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite
    and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than
    uniform scaling, providing both efficiency gains and an interpretable decision
    rule.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12276
  arxiv_url: http://arxiv.org/abs/2602.12276
  key_findings: 'Test-time scaling has become a standard way to improve performance
    and boost reliability of neural network models. However, its behavior on agentic,
    multi-step tasks remains less well-understood: small per-step errors can compound
    over long horizons; and we find that naive policies that uniformly increase sampling
    show diminishing returns. In this work, we present CATTS, a simple technique for
    dynamically allocating compute for multi-step agents.'
- id: '2602.12270'
  title: Creative Ownership in the Age of AI
  authors: Annie Liang, Jay Lu
  abstract: 'Copyright law focuses on whether a new work is "substantially similar"
    to an existing one, but generative AI can closely imitate style without copying
    content, a capability now central to ongoing litigation. We argue that existing
    definitions of infringement are ill-suited to this setting and propose a new criterion:
    a generative AI output infringes on an existing work if it could not have been
    generated without that work in its training corpus. To operationalize this definition,
    we model generative systems as closure operators mapping a corpus of existing
    works to an output of new works. AI generated outputs are \emph{permissible} if
    they do not infringe on any existing work according to our criterion. Our results
    characterize structural properties of permissible generation and reveal a sharp
    asymptotic dichotomy: when the process of organic creations is light-tailed, dependence
    on individual works eventually vanishes, so that regulation imposes no limits
    on AI generation; with heavy-tailed creations, regulation can be persistently
    constraining.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12270
  arxiv_url: http://arxiv.org/abs/2602.12270
  key_findings: 'Copyright law focuses on whether a new work is "substantially similar"
    to an existing one, but generative AI can closely imitate style without copying
    content, a capability now central to ongoing litigation. We argue that existing
    definitions of infringement are ill-suited to this setting and propose a new criterion:
    a generative AI output infringes on an existing work if it could not have been
    generated without that work in its training corpus. To operationalize this definition,
    we model generative systems as closure operators mapping a corpus of existing
    works to an output of new works.'
- id: '2602.12268'
  title: 'CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step
    Agentic Tool Use'
  authors: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang
    Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin
    Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
  abstract: 'AI agents are increasingly used to solve real-world tasks by reasoning
    over multi-turn user interactions and invoking external tools. However, applying
    reinforcement learning to such settings remains difficult: realistic objectives
    often lack verifiable rewards and instead emphasize open-ended behaviors; moreover,
    RL for multi-turn, multi-step agentic tool use is still underexplored; and building
    and maintaining executable tool environments is costly, limiting scale and coverage.
    We propose CM2, an RL framework that replaces verifiable outcome rewards with
    checklist rewards. CM2 decomposes each turn''s intended behavior into fine-grained
    binary criteria with explicit evidence grounding and structured metadata, turning
    open-ended judging into more stable classification-style decisions. To balance
    stability and informativeness, our method adopts a strategy of sparse reward assignment
    but dense evaluation criteria. Training is performed in a scalable LLM-simulated
    tool environment, avoiding heavy engineering for large tool sets. Experiments
    show that CM2 consistently improves over supervised fine-tuning. Starting from
    an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the
    SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12
    points on ToolSandbox. The results match or even outperform similarly sized open-source
    baselines, including the judging model. CM2 thus provides a scalable recipe for
    optimizing multi-turn, multi-step tool-using agents without relying on verifiable
    rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12268
  arxiv_url: http://arxiv.org/abs/2602.12268
  key_findings: 'AI agents are increasingly used to solve real-world tasks by reasoning
    over multi-turn user interactions and invoking external tools. However, applying
    reinforcement learning to such settings remains difficult: realistic objectives
    often lack verifiable rewards and instead emphasize open-ended behaviors; moreover,
    RL for multi-turn, multi-step agentic tool use is still underexplored; and building
    and maintaining executable tool environments is costly, limiting scale and coverage.
    We propose CM2, an RL framework that replaces verifiable outcome rewards with
    checklist rewards.'
- id: '2602.12259'
  title: 'Think like a Scientist: Physics-guided LLM Agent for Equation Discovery'
  authors: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama,
    Rose Yu
  abstract: 'Explaining observed phenomena through symbolic, interpretable formulas
    is a fundamental goal of science. Recently, large language models (LLMs) have
    emerged as promising tools for symbolic equation discovery, owing to their broad
    domain knowledge and strong reasoning capabilities. However, most existing LLM-based
    systems try to guess equations directly from data, without modeling the multi-step
    reasoning process that scientists often follow: first inferring physical properties
    such as symmetries, then using these as priors to restrict the space of candidate
    equations. We introduce KeplerAgent, an agentic framework that explicitly follows
    this scientific reasoning process. The agent coordinates physics-based tools to
    extract intermediate structure and uses these results to configure symbolic regression
    engines such as PySINDy and PySR, including their function libraries and structural
    constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves
    substantially higher symbolic accuracy and greater robustness to noisy data than
    both LLM and traditional baselines.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12259
  arxiv_url: http://arxiv.org/abs/2602.12259
  key_findings: 'Explaining observed phenomena through symbolic, interpretable formulas
    is a fundamental goal of science. Recently, large language models (LLMs) have
    emerged as promising tools for symbolic equation discovery, owing to their broad
    domain knowledge and strong reasoning capabilities. However, most existing LLM-based
    systems try to guess equations directly from data, without modeling the multi-step
    reasoning process that scientists often follow: first inferring physical properties
    such as symmetries, then using these as priors to restrict the space of candidate
    equations.'
- id: '2602.12257'
  title: On the implicit regularization of Langevin dynamics with projected noise
  authors: Govind Menon, Austin J. Stromme, Adrien Vacher
  abstract: 'We study Langevin dynamics with noise projected onto the directions orthogonal
    to an isometric group action. This mathematical model is introduced to shed new
    light on the effects of symmetry on stochastic gradient descent for over-parametrized
    models. Our main result identifies a novel form of implicit regularization: when
    the initial and target density are both invariant under the group action, Langevin
    dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic
    diffusion but with an additional drift term proportional to the negative log volume
    of the group orbit. We prove this result by constructing a coupling of the two
    processes via a third process on the group itself, and identify the additional
    drift as the mean curvature of the orbits.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12257
  arxiv_url: http://arxiv.org/abs/2602.12257
  key_findings: 'We study Langevin dynamics with noise projected onto the directions
    orthogonal to an isometric group action. This mathematical model is introduced
    to shed new light on the effects of symmetry on stochastic gradient descent for
    over-parametrized models. Our main result identifies a novel form of implicit
    regularization: when the initial and target density are both invariant under the
    group action, Langevin dynamics with projected noise is equivalent in law to Langevin
    dynamics with isotropic diffusion but with an additional drift term proportional
    to the negative log volume of the group orbit.'
- id: '2602.12251'
  title: A technical curriculum on language-oriented artificial intelligence in translation
    and specialised communication
  authors: Ralph Kr√ºger
  abstract: This paper presents a technical curriculum on language-oriented artificial
    intelligence (AI) in the language and translation (L&T) industry. The curriculum
    aims to foster domain-specific technical AI literacy among stakeholders in the
    fields of translation and specialised communication by exposing them to the conceptual
    and technical/algorithmic foundations of modern language-oriented AI in an accessible
    way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations
    of neural networks, 3) tokenization and 4) transformer neural networks. It is
    intended to help users develop computational thinking as well as algorithmic awareness
    and algorithmic agency, ultimately contributing to their digital resilience in
    AI-driven work environments. The didactic suitability of the curriculum was tested
    in an AI-focused MA course at the Institute of Translation and Multilingual Communication
    at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but
    participant feedback indicates that it should be embedded into higher-level didactic
    scaffolding - e.g., in the form of lecturer support - in order to enable optimal
    learning conditions.
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12251
  arxiv_url: http://arxiv.org/abs/2602.12251
  key_findings: This paper presents a technical curriculum on language-oriented artificial
    intelligence (AI) in the language and translation (L&T) industry. The curriculum
    aims to foster domain-specific technical AI literacy among stakeholders in the
    fields of translation and specialised communication by exposing them to the conceptual
    and technical/algorithmic foundations of modern language-oriented AI in an accessible
    way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations
    of neural networks, 3) tokenization and 4) transformer neural networks.
- id: '2602.12249'
  title: '"Sorry, I Didn''t Catch That": How Speech Models Miss What Matters Most'
  authors: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou
  abstract: 'Despite speech recognition systems achieving low word error rates on
    standard benchmarks, they often fail on short, high-stakes utterances in real-world
    deployments. Here, we study this failure mode in a high-stakes task: the transcription
    of U.S. street names as spoken by U.S. participants. We evaluate 15 models from
    OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse
    U.S. speakers and find an average transcription error rate of 44%. We quantify
    the downstream impact of failed transcriptions by geographic locations and show
    that mis-transcriptions systematically cause errors for all speakers, but that
    routing distance errors are twice as large for non-English primary speakers compared
    to English primary speakers. To mitigate this harm, we introduce a synthetic data
    generation approach that produces diverse pronunciations of named entities using
    open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic
    samples improves street name transcription accuracy by nearly 60% (relative to
    base models) for non-English primary speakers. Our results highlight a critical
    gap between benchmark performance and real-world reliability in speech systems
    and demonstrate a simple, scalable path to reducing high-stakes transcription
    errors.'
  published: '2026-02-12'
  pdf_url: http://arxiv.org/pdf/2602.12249
  arxiv_url: http://arxiv.org/abs/2602.12249
  key_findings: 'Despite speech recognition systems achieving low word error rates
    on standard benchmarks, they often fail on short, high-stakes utterances in real-world
    deployments. Here, we study this failure mode in a high-stakes task: the transcription
    of U. S.'
